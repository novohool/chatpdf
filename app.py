import streamlit as st
import requests
import json
import os
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.memory import ChatMessageHistory
from langchain.schema import HumanMessage, AIMessage

def get_current_session():
    return st.session_state.session_id

def get_new_session(session_id='default_chat'):
    st.session_state.session_id = session_id

def get_session_history(session_id: str) -> ChatMessageHistory:
    session_id = get_current_session() if not session_id else session_id
    if session_id not in st.session_state.store:
        st.session_state.store[session_id] = ChatMessageHistory()
    return st.session_state.store[session_id]

def start_new_chat():
    new_session_id = f'chat_{len(st.session_state.store) + 1}'
    get_new_session(new_session_id)
    st.session_state.query = ""
    st.session_state.store[new_session_id] = ChatMessageHistory()

def initialize_session_state():
    if 'store' not in st.session_state:
        st.session_state.store = {}
    if 'session_id' not in st.session_state:
        get_new_session()
    if 'query' not in st.session_state:
        st.session_state.query = ""
    if 'button_key' not in st.session_state:
        st.session_state.button_key = 0

class LlamaChat:
    def __init__(self):
        st.set_page_config(page_title="Llama Chat", page_icon="ğŸ¦™")
        st.title("Llama Chat")
        st.write("ä¸ Llama æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œè·å–å®æ—¶å“åº”ã€‚")
        
        initialize_session_state()

        if 'vectorstore' not in st.session_state:
            st.session_state.vectorstore = None

        self.add_custom_css()

    def add_custom_css(self):
        custom_css = """
        <style>
        .stButton button {
            background-color: #4CAF50;
            color: white;
            padding: 10px 20px;
            border: none;
            cursor: pointer;
            transition: transform 0.2s ease, background-color 0.3s ease;
            border-radius: 0;
        }
        .stButton button:hover {
            background-color: #45a049;
            transform: scale(1.05);
        }
        .stTextInput input, .stTextArea textarea {
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 0;
            transition: border-color 0.3s ease;
        }
        .stTextInput input:focus, .stTextArea textarea:focus {
            border-color: #4CAF50;
            outline: none;
        }
        .stMarkdown p {
            background-color: #f7f7f7;
            padding: 10px;
            margin: 10px 0;
            border-radius: 5px;
            border: none;
        }
        .stForm {
            padding: 20px;
            background-color: white;
            box-shadow: none;
            border-radius: 0;
        }
        .stSpinner {
            font-size: 16px;
            color: #4CAF50;
        }
        </style>
        """
        st.markdown(custom_css, unsafe_allow_html=True)

    @st.cache_resource(ttl="1h", show_spinner='Processing File(s).', hash_funcs={"_io.BytesIO": lambda _: None, "__main__.LlamaChat": lambda _: None,"langchain.chains.RetrievalQA": lambda _: None,"PyPDFLoader": lambda _: None})
    def get_docs_from_files(self, files):
        documents = []
        for file in files:
            filepath = file.name
            with open(filepath, "wb") as f:
                f.write(file.getvalue())
    
            try:
                docs = PyPDFLoader(filepath).load()
                documents.extend(docs)
            except UnicodeDecodeError:
                st.error(f"æ–‡ä»¶ {filepath} åŒ…å«é UTF-8 å­—ç¬¦ï¼Œæ— æ³•è¯»å–ã€‚")
    
            if os.path.exists(filepath):
                os.remove(filepath)
    
        return documents
        
    @st.cache_resource(ttl="1h", show_spinner='Processing File(s)..', hash_funcs={"_io.BytesIO": lambda _: None, "__main__.LlamaChat": lambda _: None,"langchain.chains.RetrievalQA": lambda _: None,"PyPDFLoader": lambda _: None})
    def get_vectorstore_from_files(self, files, HF_Embed_Model):
        pdf_docs = self.get_docs_from_files(files)             
        split_docs = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150).split_documents(pdf_docs)
        embeddings = HuggingFaceEmbeddings(model_name=HF_Embed_Model)
        vectorstore = FAISS.from_documents(split_docs, embeddings)
    
        return vectorstore
    @st.cache_resource(show_spinner='Processing streamed data ...', hash_funcs={"_io.BytesIO": lambda _: None, "__main__.LlamaChat": lambda _: None,"langchain.chains.RetrievalQA": lambda _: None,"PyPDFLoader": lambda _: None})
    def get_streamed_data(self, user_input):
        url = "https://llama3.bnnd.eu.org/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        history = get_session_history(get_current_session()).messages
        valid_roles = {"system", "user", "assistant"}
        data = {
            "model": "llama-3.1-405b",
            "stream": True,
            "messages": [
                {"role": "system", "content": "ç”¨ä¸­æ–‡å›ç­”"}
            ] + [{"role": msg.role, "content": msg.content} for msg in history if isinstance(msg, (HumanMessage, AIMessage)) and msg.role in valid_roles] + [{"role": "user", "content": user_input}]
        }

        try:
            with requests.post(url, headers=headers, json=data, stream=True) as response:
                response_text = ""
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8', errors='ignore')
                        if decoded_line.startswith("data: "):
                            json_data = decoded_line[6:]
                            if json_data == "[DONE]":
                                break
                            try:
                                chunk = json.loads(json_data)
                                if "choices" in chunk and chunk["choices"]:
                                    content = chunk["choices"][0]["delta"].get("content", "")
                                    response_text += content
                            except json.JSONDecodeError:
                                continue
                return response_text
        except requests.RequestException as e:
            st.error(f"è¯·æ±‚å¤±è´¥: {e}")
            return None

    def display_history(self):
        st.write("å†å²æ¶ˆæ¯:")
        history = get_session_history(get_current_session()).messages
        for message in history:
            if isinstance(message, HumanMessage):
                st.markdown(f"**ç”¨æˆ·ï¼š** {message.content}")
            elif isinstance(message, AIMessage):
                st.markdown(f"**åŠ©æ‰‹ï¼š** {message.content}")

    @st.cache_resource(show_spinner='Processing rag chain ....', hash_funcs={"_io.BytesIO": lambda _: None, "__main__.LlamaChat": lambda _: None,"langchain.chains.RetrievalQA": lambda _: None,"PyPDFLoader": lambda _: None})
    def rag_chain(self, user_input):
        retriever = st.session_state.vectorstore.as_retriever()
        response_text = self.get_streamed_data(user_input)
        if response_text:
            return response_text
        else:
            return "No response received."

    def main_fragment(self):
        with st.form(key=f"form_{st.session_state.button_key}", clear_on_submit=False):
            user_input = st.text_area("è¾“å…¥ä½ çš„é—®é¢˜:", "", key=f"input_{st.session_state.button_key}")
            submit_button = st.form_submit_button(label="å‘é€")

            if submit_button:
                if user_input.strip() == "":
                    st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚")
                else:
                    with st.spinner("æ­£åœ¨å¤„ç†..."):
                        final_response = self.rag_chain(user_input)
                        if final_response:
                            history = get_session_history(get_current_session())
                            history.add_message(HumanMessage(content=user_input))
                            history.add_message(AIMessage(content=final_response))
                            st.markdown(final_response)
                            st.success("å¤„ç†å®Œæˆ!")
                            st.session_state.button_key += 1
                        else:
                            st.error("å¤„ç†å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚")

        self.display_history()

    def main(self):
        files = st.file_uploader('Choose PDF file(s)', type=['pdf'], accept_multiple_files=True)
        if files:
            HF_Embed_Model = "sentence-transformers/all-MiniLM-L6-v2"
            vectorstore = self.get_vectorstore_from_files(files=files, HF_Embed_Model=HF_Embed_Model)
            st.session_state.vectorstore = vectorstore  # å­˜å‚¨ vectorstore åˆ° session_state
            st.success("å‘é‡å­˜å‚¨å·²ç”Ÿæˆï¼")

        self.main_fragment()

if __name__ == "__main__":
    chat = LlamaChat()
    chat.main()
